# OpenAI API Configuration
# Copy this file to .env and fill in your values

# API Key (required)
OPENAI_API_KEY=your-api-key-here

# Base URL (optional - for Gemini or other OpenAI-compatible APIs)
# Leave empty for default OpenAI API
# For Gemini: https://generativelanguage.googleapis.com/v1beta/openai/
OPENAI_BASE_URL=

# Model to use (optional - can override with --model flag)
DEFAULT_MODEL=gpt-4o-mini

# Temperature (optional - can override with --temperature flag)
DEFAULT_TEMPERATURE=0.3

# Limit number of conversations to process (optional)
# Leave empty to process all conversations
# Set a number to limit processing (useful for testing or large datasets)
PROCESS_LIMIT=

# Example configurations:

## For OpenAI GPT-4o-mini:
# OPENAI_API_KEY=sk-proj-xxxxx
# OPENAI_BASE_URL=
# DEFAULT_MODEL=gpt-4o-mini

## For Gemini via OpenAI-compatible endpoint:
# OPENAI_API_KEY=your-gemini-api-key
# OPENAI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# DEFAULT_MODEL=gemini-1.5-flash

## For local models (like via LiteLLM):
# OPENAI_API_KEY=dummy
# OPENAI_BASE_URL=http://localhost:8000/v1
# DEFAULT_MODEL=local-model-name

