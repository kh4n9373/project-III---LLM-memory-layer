from __future__ import annotations

import argparse
import csv
import json
import math
import os
import re
import unicodedata
from typing import Any, Dict, List, Tuple

try:
    from tqdm import tqdm
except Exception:  # pragma: no cover
    tqdm = None

METRIC_KEYS = ["precision", "recall", "f1", "ndcg"]
DEFAULT_OUT_DIR = "/home/hungpv/projects/conversation_magix/eval_results"
ALL_K_SENTINEL = 10**9  # d√πng l√†m key cho @ALL


# ========================= TEXT UTILS =========================
def _normalize(t: str) -> str:
    t = unicodedata.normalize("NFKC", t)
    t = (
        t.replace("‚Äô", "'")
        .replace("‚Äú", '"')
        .replace("‚Äù", '"')
        .replace("\u00A0", " ")
    )
    t = re.sub(r"\s+", " ", t).strip().lower()
    return t


def _strip_speakers(t: str) -> str:
    """B·ªè 'User:' / 'Assistant:' n·∫øu c√≥."""
    return re.sub(r"\b(user|assistant)\s*:\s*", "", t, flags=re.I)


def _contains_or_sim(chunk: str, ev: str, contain_threshold: float = 0.85) -> bool:
    """
    Match nh·ªã ph√¢n: true n·∫øu evidence l√† substring c·ªßa chunk
    ho·∫∑c c√°c token c·ªßa evidence xu·∫•t hi·ªán >= contain_threshold trong chunk.
    """
    if ev in chunk:
        return True
    ev_toks = re.findall(r"\w+", ev)
    ch_tokens = set(re.findall(r"\w+", chunk))
    if not ev_toks:
        return False
    inter = sum(1 for w in ev_toks if w in ch_tokens)
    return (inter / len(ev_toks)) >= contain_threshold


# ========================= HELPERS =========================
def _coerce_chunks(item: Dict[str, Any]) -> List[str]:
    """
    Chu·∫©n h√≥a item['chunks'] v·ªÅ List[str].
    - N·∫øu l√† list[str] -> gi·ªØ nguy√™n
    - N·∫øu l√† list[dict] -> ∆∞u ti√™n c√°c key ph·ªï bi·∫øn
    """
    raw = item.get("chunks", []) or []
    out: List[str] = []
    for c in raw:
        if isinstance(c, str):
            out.append(c)
        elif isinstance(c, dict):
            for key in ("chunk_content", "content", "text", "raw", "value"):
                if key in c and isinstance(c[key], str):
                    out.append(c[key])
                    break
    return out


def _coerce_evidences(item: Dict[str, Any]) -> List[str]:
    if 'evidence' in item:
        ev = item.get("evidence", [])
    elif 'evidences' in item:
        ev = item.get("evidences", [])
    else:
        ev = []

    return [e for e in ev if isinstance(e, str)]


def _has_evidence(item: Dict[str, Any]) -> bool:
    return bool(_coerce_evidences(item))


def _parse_ks(ks_str: str) -> Tuple[Tuple[int, ...], bool]:
    """
    Tr·∫£ v·ªÅ (ks, use_all). H·ªó tr·ª£ 'all' ho·∫∑c '0' ƒë·ªÉ d√πng to√†n b·ªô chunks.
    """
    parts = [x.strip().lower() for x in ks_str.split(",")]
    use_all = any(x in ("all", "0") for x in parts)
    ks = tuple(sorted({int(x) for x in parts if x.isdigit() and int(x) > 0}))
    return ks, use_all


# ========================= LOW-LEVEL COUNTS =========================
def _evaluate_counts(
    chunks: List[str],
    evidences: List[str],
    k: int | None = None,
    contain_threshold: float = 0.85,
) -> Dict[str, float]:
    """
    Tr·∫£ v·ªÅ c√°c 'ƒë·∫øm' ƒë·ªÉ c√≥ th·ªÉ c·ªông d·ªìn micro:
      - tp_evidence: s·ªë evidence ƒë∆∞·ª£c cover (ƒë·ªÉ t√≠nh recall)
      - retrieved:   s·ªë chunk ƒë∆∞·ª£c x√©t (sau khi c·∫Øt k)
      - gold:        t·ªïng #evidence
      - rel_chunks:  s·ªë chunk 'relevant' (match >=1 evidence) ‚Äî ƒë·ªÉ t√≠nh precision ki·ªÉu IR
      - dcg / idcg:  cho nDCG
    """
    if k is not None:
        chunks = chunks[:k]

    chunks_n = [_normalize(_strip_speakers(c)) for c in chunks]
    evidences_n = [_normalize(_strip_speakers(e)) for e in evidences]

    hit_evs = set()
    rel_chunk_count = 0
    dcg = 0.0

    for rank, ch in enumerate(chunks_n, start=1):
        matched = False
        for ev in evidences_n:
            if _contains_or_sim(ch, ev, contain_threshold=contain_threshold):
                matched = True
                hit_evs.add(ev)
        if matched:
            rel_chunk_count += 1
            dcg += 1.0 / math.log2(rank + 1)

    tp_ev = float(len(hit_evs))
    retrieved = float(len(chunks_n))
    gold = float(len(evidences_n))
    # IDCG d·ª±a tr√™n s·ªë chunk li√™n quan t·ªëi ƒëa c√≥ th·ªÉ x·∫øp ·ªü top (<= retrieved),
    # d√πng rel_chunk_count ƒë·ªÉ tr√°nh nDCG > 1 khi s·ªë chunk li√™n quan > s·ªë evidence
    ideal_hits = int(min(rel_chunk_count, int(retrieved)))
    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, ideal_hits + 1)) if ideal_hits else 0.0

    return {
        "tp_evidence": tp_ev,
        "retrieved": retrieved,
        "gold": gold,
        "rel_chunks": float(rel_chunk_count),
        "dcg": float(dcg),
        "idcg": float(idcg),
    }


def _counts_to_metrics(cnt: Dict[str, float], precision_mode: str = "ir") -> Dict[str, float]:
    """
    precision_mode:
      - "ir"     : precision = rel_chunks / retrieved  (chu·∫©n IR, khuy·∫øn ngh·ªã)
      - "legacy" : precision = tp_evidence / retrieved (gi·ªëng c√°ch c≈©)
    """
    if cnt["retrieved"] > 0:
        if precision_mode == "legacy":
            precision = cnt["tp_evidence"] / cnt["retrieved"]
        else:
            precision = cnt["rel_chunks"] / cnt["retrieved"]
    else:
        precision = 0.0

    recall = (cnt["tp_evidence"] / cnt["gold"]) if cnt["gold"] > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    ndcg = (cnt["dcg"] / cnt["idcg"]) if cnt["idcg"] > 0 else 0.0
    return {"precision": precision, "recall": recall, "f1": f1, "ndcg": ndcg}


# ========================= EVAL CORE =========================
def eval_one_record(
    item: Dict[str, Any],
    ks: Tuple[int, ...],
    use_all: bool,
    contain_threshold: float,
    precision_mode: str,
) -> Tuple[Dict[int, Dict[str, float]], Dict[int, Dict[str, float]]]:
    """
    Tr·∫£ v·ªÅ:
      - per_k_metrics: metrics theo k (macro ƒë∆°n v·ªã 1 record)
      - per_k_counts : counts theo k (ƒë·ªÉ c·ªông d·ªìn l√†m micro)
    """
    chunks = _coerce_chunks(item)
    evidences = _coerce_evidences(item)

    per_k_metrics: Dict[int, Dict[str, float]] = {}
    per_k_counts: Dict[int, Dict[str, float]] = {}

    # @ALL
    if use_all:
        cnt_all = _evaluate_counts(chunks, evidences, k=None, contain_threshold=contain_threshold)
        per_k_counts[ALL_K_SENTINEL] = cnt_all
        per_k_metrics[ALL_K_SENTINEL] = _counts_to_metrics(cnt_all, precision_mode=precision_mode)

    # @k c·ª• th·ªÉ
    for k in ks:
        cnt = _evaluate_counts(chunks, evidences, k=k, contain_threshold=contain_threshold)
        per_k_counts[k] = cnt
        per_k_metrics[k] = _counts_to_metrics(cnt, precision_mode=precision_mode)

    return per_k_metrics, per_k_counts


def eval_dataset(
    dataset: List[Dict[str, Any]],
    ks: Tuple[int, ...] = (3, 5, 10),
    use_all: bool = False,
    contain_threshold: float = 0.85,
    precision_mode: str = "ir",
):
    # macro sums
    macro_sums = {k: {m: 0.0 for m in METRIC_KEYS} for k in ks}
    counts = {k: 0 for k in ks}
    if use_all:
        macro_sums[ALL_K_SENTINEL] = {m: 0.0 for m in METRIC_KEYS}
        counts[ALL_K_SENTINEL] = 0

    failed: List[Tuple[int, str]] = []
    skipped_no_evidence: List[Tuple[int, str]] = []  # (idx, qid)
    per_record = {k: [] for k in ks}
    if use_all:
        per_record[ALL_K_SENTINEL] = []

    # micro sums
    micro_sums = {
        k: {"tp_evidence": 0.0, "retrieved": 0.0, "gold": 0.0, "rel_chunks": 0.0, "dcg": 0.0, "idcg": 0.0}
        for k in ks
    }
    if use_all:
        micro_sums[ALL_K_SENTINEL] = {"tp_evidence": 0.0, "retrieved": 0.0, "gold": 0.0,
                                      "rel_chunks": 0.0, "dcg": 0.0, "idcg": 0.0}

    iterator = range(len(dataset))
    if tqdm is not None:
        iterator = tqdm(iterator, total=len(dataset), desc="Eval records")

    for idx in iterator:
        item = dataset[idx]
        qid = item.get("question_id", f"idx{idx}")

        # Skip record kh√¥ng c√≥ evidences
        if not _has_evidence(item):
            skipped_no_evidence.append((idx, qid))
            continue

        try:
            per_k_metrics, per_k_counts = eval_one_record(
                item=item,
                ks=ks,
                use_all=use_all,
                contain_threshold=contain_threshold,
                precision_mode=precision_mode,
            )
        except Exception as e:
            failed.append((idx, repr(e)))
            continue

        for k, m in per_k_metrics.items():
            per_record.setdefault(k, []).append((idx, qid, m))
            for mk in METRIC_KEYS:
                macro_sums[k][mk] += m.get(mk, 0.0)
            counts[k] = counts.get(k, 0) + 1

        for k, c in per_k_counts.items():
            ms = micro_sums.setdefault(k, {"tp_evidence": 0.0, "retrieved": 0.0, "gold": 0.0,
                                           "rel_chunks": 0.0, "dcg": 0.0, "idcg": 0.0})
            for key in ms:
                ms[key] += c[key]

    macro_avgs = {
        k: {m: (macro_sums[k][m] / counts[k]) if counts[k] > 0 else 0.0 for m in METRIC_KEYS}
        for k in counts.keys()
    }

    micro_avgs = {k: _counts_to_metrics(c, precision_mode=precision_mode) for k, c in micro_sums.items()}

    # tr·∫£ th√™m danh s√°ch skip
    return macro_avgs, micro_avgs, counts, failed, per_record, micro_sums, skipped_no_evidence


# ========================= REPORT/DUMP =========================
def _k_label(k: int) -> str:
    return "ALL" if k == ALL_K_SENTINEL else str(k)


def print_report_both(macro_avgs, micro_avgs, counts, failed, skipped_no_evidence):
    print("=== Retrieval evaluation ===")
    for k in sorted(counts.keys()):
        ma = macro_avgs[k]
        mi = micro_avgs[k]
        label = _k_label(k)
        print(f"--- @ {label} (n={counts[k]}) ---")
        print(f"Macro  | P: {ma['precision']:.4f}  R: {ma['recall']:.4f}  F1: {ma['f1']:.4f}  nDCG: {ma['ndcg']:.4f}")
        print(f"Micro  | P: {mi['precision']:.4f}  R: {mi['recall']:.4f}  F1: {mi['f1']:.4f}  nDCG: {mi['ndcg']:.4f}")

    if skipped_no_evidence:
        print(f"\n[Info] Skipped {len(skipped_no_evidence)} record(s) without evidences.")

    if failed:
        print(f"[Warn] {len(failed)} record(s) l·ªói, b·ªè qua:")
        for i, (idx, err) in enumerate(failed[:10], start=1):
            print(f"  {i}. index={idx}, err={err}")
        if len(failed) > 10:
            print(f"  ... v√† {len(failed) - 10} l·ªói kh√°c")


def dump_bad_cases(
    dataset: List[Dict[str, Any]],
    per_record: dict,
    out_path: str,
    ks: Tuple[int, ...],
    thresholds: Dict[str, float] | None = None,
    bottoms: Dict[str, int] | None = None,
    include_question: bool = True,
    use_all: bool = False,
):
    thresholds = thresholds or {}
    bottoms = bottoms or {}
    by_k = {}

    all_ks = list(ks)
    if use_all:
        all_ks.append(ALL_K_SENTINEL)

    for k in all_ks:
        arr = per_record.get(k, [])
        selected = []

        for metric, thr in thresholds.items():
            for idx, qid, m in arr:
                if m.get(metric, 0.0) < float(thr):
                    selected.append(("thresh", metric, thr, idx, qid, m))

        for metric, n in bottoms.items():
            worst = sorted(arr, key=lambda t: t[2].get(metric, 0.0))[:int(n)]
            for idx, qid, m in worst:
                selected.append(("bottom", metric, n, idx, qid, m))

        seen = set()
        items = []
        for kind, metric, param, idx, qid, m in selected:
            key = (idx, qid)
            if key in seen:
                continue
            seen.add(key)
            entry = {
                "idx": idx,
                "question_id": qid,
                "reason": (
                    f"{kind}:{metric}<{param}" if kind == "thresh" else f"{kind}:{metric}@{param}"
                ),
                "metrics": {mk: float(m.get(mk, 0.0)) for mk in METRIC_KEYS},
            }
            if include_question:
                entry["question"] = dataset[idx].get("question")
            items.append(entry)

        by_k[_k_label(k)] = items

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    payload = {
        "config": {"thresholds": thresholds, "bottoms": bottoms},
        "counts": {str(_k_label(k)): len(by_k[_k_label(k)]) for k in by_k.keys()},
        "by_k": by_k,
    }
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ Dumped bad cases to: {out_path}")


def dump_bad_cases_csv(json_path: str, csv_path: str):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    rows = []
    for k, items in data["by_k"].items():
        for it in items:
            rows.append({
                "k": k,
                "idx": it["idx"],
                "question_id": it["question_id"],
                "reason": it["reason"],
                "precision": it["metrics"]["precision"],
                "recall": it["metrics"]["recall"],
                "f1": it["metrics"]["f1"],
                "ndcg": it["metrics"]["ndcg"],
                "question": it.get("question", ""),
            })

    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        if rows:
            writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))
            writer.writeheader()
            writer.writerows(rows)
        else:
            f.write("")
    print(f"‚úÖ CSV written: {csv_path}")


# ========================= SAVE RESULTS =========================
def save_eval_results(
    out_dir: str,
    out_filename: str,
    meta: Dict[str, Any],
    macro_avgs: Dict[int, Dict[str, float]],
    micro_avgs: Dict[int, Dict[str, float]],
    counts: Dict[int, int],
    failed: List[Tuple[int, str]],
    per_record: Dict[int, List[Tuple[int, str, Dict[str, float]]]],
    micro_sums: Dict[int, Dict[str, float]],
    skipped_no_evidence: List[Tuple[int, str]],
) -> str:
    """
    L∆∞u file JSON k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß (meta + macro/micro + per-record + micro_sums + skipped list).
    out_filename n√™n l√† basename c·ªßa file input ƒë·ªÉ "gi·ªëng t√™n file input".
    """
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, out_filename)

    def _dict_with_str_k(d: Dict[int, Any]) -> Dict[str, Any]:
        return {_k_label(k): v for k, v in d.items()}

    payload: Dict[str, Any] = {
        "meta": meta,
        "macro_avgs": _dict_with_str_k(macro_avgs),
        "micro_avgs": _dict_with_str_k(micro_avgs),
        "counts": _dict_with_str_k(counts),
        "failed": failed,
        "per_record": {
            _k_label(k): [
                {"idx": idx, "question_id": qid, "metrics": m}
                for (idx, qid, m) in arr
            ]
            for k, arr in per_record.items()
        },
        "micro_sums": _dict_with_str_k(micro_sums),  # ƒë·ªÉ ki·ªÉm tra l·∫°i ph√©p t√≠nh micro
        "skipped_no_evidence": [{"idx": i, "question_id": q} for (i, q) in skipped_no_evidence],
    }
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    print(f"üëå Saved eval results -> {out_path}")
    return out_path


# ========================= CLI =========================
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Evaluate given chunks vs evidences (no retrieve)")
    p.add_argument("--input", nargs="+", required=True,
                   help="Path(s) to dataset json (s·∫Ω ch·∫°y & l∆∞u t·ª´ng file t∆∞∆°ng ·ª©ng)")
    p.add_argument("--ks", default="3,5,10",
                   help="Comma list, vd: 1,3,5,10 ho·∫∑c th√™m 'all' hay '0' ƒë·ªÉ d√πng to√†n b·ªô chunks")
    p.add_argument("--contain-threshold", type=float, default=0.85)
    p.add_argument("--precision-mode", choices=["ir", "legacy"], default="ir",
                   help="precision 'ir' = rel_chunks/retrieved (chu·∫©n), 'legacy' = tp_evidence/retrieved")

    p.add_argument("--bad-json", default=None, help="Path to dump bad cases JSON")
    p.add_argument("--bad-csv", default=None, help="(Optional) Also dump CSV from bad-json")
    p.add_argument("--thr-recall", type=float, default=None, help="Ch·ªçn record c√≥ recall<thr")
    p.add_argument("--thr-f1", type=float, default=None, help="Ch·ªçn bottom theo f1<thr")
    p.add_argument("--bottom-f1", type=int, default=None, help="Ch·ªçn bottom-N theo f1")

    p.add_argument("--out-dir", default=DEFAULT_OUT_DIR,
                   help=f"Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh: {DEFAULT_OUT_DIR})")
    p.add_argument("--out", dest="out_file", default=None,
                   help="ƒê∆∞·ªùng d·∫´n FILE JSON output c·ª• th·ªÉ (d√πng khi --input c√≥ 1 file).")
    return p.parse_args()


def load_dataset(path: str) -> List[Dict[str, Any]]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def main():
    args = parse_args()
    ks, use_all = _parse_ks(args.ks)

    # Ch·∫°y & l∆∞u ri√™ng cho t·ª´ng input ƒë·ªÉ t√™n file output tr√πng v·ªõi input
    for pth in args.input:
        dataset = load_dataset(pth)
        macro_avgs, micro_avgs, counts, failed, per_record, micro_sums, skipped_no_evidence = eval_dataset(
            dataset=dataset,
            ks=ks,
            use_all=use_all,
            contain_threshold=args.contain_threshold,
            precision_mode=args.precision_mode,
        )

        print(f"\n=== File: {pth} ===")
        print_report_both(macro_avgs, micro_avgs, counts, failed, skipped_no_evidence)

        meta = {
            "input_file": pth,
            "ks": [(_k_label(k)) for k in counts.keys()],
            "contain_threshold": args.contain_threshold,
            "precision_mode": args.precision_mode,
            "skipped_no_evidence": len(skipped_no_evidence),
        }
        if args.out_file and len(args.input) > 1:
            raise SystemExit("‚ùå --out ch·ªâ d√πng khi --input c√≥ ƒë√∫ng 1 file. H√£y d√πng --out-dir cho nhi·ªÅu input.")

        if args.out_file:
            out_dir = os.path.dirname(args.out_file) or "."
            out_filename = os.path.basename(args.out_file)
            if not out_filename.lower().endswith(".json"):
                out_filename += ".json"
        else:
            out_dir = args.out_dir
            out_filename = os.path.basename(pth)
            if not out_filename.lower().endswith(".json"):
                out_filename += ".json"

        save_eval_results(
            out_dir=out_dir,
            out_filename=out_filename,
            meta=meta,
            macro_avgs=macro_avgs,
            micro_avgs=micro_avgs,
            counts=counts,
            failed=failed,
            per_record=per_record,
            micro_sums=micro_sums,
            skipped_no_evidence=skipped_no_evidence,
        )

    # Bad-case dump (√°p d·ª•ng cho file cu·ªëi c√πng ƒë√£ ch·∫°y)
    if args.bad_json:
        thresholds = {}
        if args.thr_recall is not None:
            thresholds["recall"] = float(args.thr_recall)
        if args.thr_f1 is not None:
            thresholds["f1"] = float(args.thr_f1)
        bottoms = {}
        if args.bottom_f1 is not None:
            bottoms["f1"] = int(args.bottom_f1)
        dump_bad_cases(
            dataset=dataset,
            per_record=per_record,
            out_path=args.bad_json,
            ks=ks,
            thresholds=thresholds or None,
            bottoms=bottoms or None,
            include_question=True,
            use_all=use_all,
        )
        if args.bad_csv:
            dump_bad_cases_csv(args.bad_json, args.bad_csv)


if __name__ == "__main__":
    main()

"""
python /home/hungpv/projects/custom_hippo/evaluator.py \
  --input /home/hungpv/projects/custom_hippo/outputs/retrieved_datasets/longmemeval_0_500_v3.hippo.json \
  --ks all,3,5,10 \
  --precision-mode ir \
  --contain-threshold 0.5
python /home/hungpv/projects/custom_hippo/evaluator.py \
  --input /home/hungpv/projects/custom_hippo/outputs/retrieved_datasets/longmemeval_0_500_v3.retrieved.json \
  --ks all,3,5,10 \
  --precision-mode ir \
  --contain-threshold 0.5
python /home/hungpv/projects/custom_hippo/evaluator.py \
  --input /home/hungpv/projects/custom_hippo/outputs/retrieved_datasets/longmemeval_0_500_v3.magix.json \
  --ks all,3,5,10 \
  --precision-mode ir \
  --contain-threshold 0.5
python /home/hungpv/projects/custom_hippo/evaluator.py \
  --input /home/hungpv/projects/custom_hippo/outputs/retrieved_datasets/longmemeval_0_500_v3.magix.retrieved.json \
  --ks all,3,5,10 \
  --precision-mode ir \
  --contain-threshold 0.5

python /home/hungpv/projects/custom_hippo/evaluator.py \
  --input /home/hungpv/projects/custom_hippo/outputs/retrieved_datasets/longmemeval_0_500_v3.magix.retrieved.json \
  --ks all,3,5,10 \
  --precision-mode ir \
  --contain-threshold 0.5

"""
